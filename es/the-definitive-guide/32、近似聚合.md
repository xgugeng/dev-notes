<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [统计去重后的数据](#%E7%BB%9F%E8%AE%A1%E5%8E%BB%E9%87%8D%E5%90%8E%E7%9A%84%E6%95%B0%E6%8D%AE)
  - [百分位计算](#%E7%99%BE%E5%88%86%E4%BD%8D%E8%AE%A1%E7%AE%97)
  - [百分位等级](#%E7%99%BE%E5%88%86%E4%BD%8D%E7%AD%89%E7%BA%A7)
- [学会权衡](#%E5%AD%A6%E4%BC%9A%E6%9D%83%E8%A1%A1)
- [导航](#%E5%AF%BC%E8%88%AA)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

更加复杂的操作则需要在算法的性能和内存使用上做出权衡。对于这个问题，我们有个三角因子模型：大数据、精确性和实时性。

我们需要选择其中两项：

- 精确 + 实时

  数据可以存入单台机器的内存之中，我们可以随心所欲，使用任何想用的算法。结果会 100% 精确，响应会相对快速。

- 大数据 + 精确

  传统的 Hadoop。可以处理 PB 级的数据并且为我们提供精确的答案，但它可能需要几周的时间才能为我们提供这个答案。

- 大数据 + 实时

  近似算法为我们提供准确但不精确的结果。

Elasticsearch 目前支持两种近似算法（ `cardinality` 和 `percentiles` ）。 它们会提供准确但不是 100% 精确的结果。 以牺牲一点小小的估算错误为代价，这些算法可以为我们换来高速的执行效率和极小的内存消耗。

# 统计去重后的数据

Elasticsearch 提供的首个近似聚合是 `cardinality` （注：基数）度量。 它提供一个字段的基数，即该字段的 *distinct* 或者 *unique* 值的数目。

每月有多少颜色的车被售出？为了得到这个度量，我们只需要将一个 `cardinality` 度量嵌入一个 `date_histogram` ：

```
GET /cars/transactions/_search
{
  "size" : 0,
  "aggs" : {
      "months" : {
        "date_histogram": {
          "field": "sold",
          "interval": "month"
        },
        "aggs": {
          "distinct_colors" : {
              "cardinality" : {
                "field" : "color"
              }
          }
        }
      }
  }
}
```

`cardinality` 度量是一个近似算法。 它是基于 [HyperLogLog++](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40671.pdf) （HLL）算法的。 HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。

要配置精度，我们必须指定 `precision_threshold` 参数的值。 这个阈值定义了在何种基数水平下我们希望得到一个近乎精确的结果。参考以下示例：

```
GET /cars/transactions/_search
{
    "size" : 0,
    "aggs" : {
        "distinct_colors" : {
            "cardinality" : {
              "field" : "color",
              "precision_threshold" : 100 
            }
        }
    }
}
```

## 百分位计算

Elasticsearch 提供的另外一个近似度量就是 `percentiles` 百分位数度量。 百分位数展现某以具体百分比下观察到的数值。例如，第95个百分位上的数值，是高于 95% 的数据总和。

```
GET /website/logs/_search
{
    "size" : 0,
    "aggs" : {
        "load_times" : {
            "percentiles" : {
                "field" : "latency" 
            }
        },
        "avg_load_time" : {
            "avg" : {
                "field" : "latency" 
            }
        }
    }
}
```

## 百分位等级

这里有另外一个紧密相关的度量叫 `percentile_ranks`。 `percentiles` 度量告诉我们落在某个百分比以下的所有文档的最小值。例如，如果 50 百分位是 119ms，那么有 50% 的文档数值都不超过 119ms。 `percentile_ranks` 告诉我们某个具体值属于哪个百分位。119ms 的 `percentile_ranks` 是在 50 百分位。 这基本是个双向关系，例如：

- 50 百分位是 119ms。
- 119ms 百分位等级是 50 百分位。

```
GET /website/logs/_search
{
    "size" : 0,
    "aggs" : {
        "zones" : {
            "terms" : {
                "field" : "zone"
            },
            "aggs" : {
                "load_times" : {
                    "percentile_ranks" : {
                      "field" : "latency",
                      "values" : [210, 800] 
                    }
                }
            }
        }
    }
}
```

# 学会权衡

取而代之的是 `percentiles` 使用一个 TDigest 算法，（由 Ted Dunning 在 [Computing Extremely Accurate Quantiles Using T-Digests](https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf) 里面提出的）。 与 HyperLogLog 一样，不需要理解完整的技术细节，但有必要了解算法的特性：

- 百分位的准确度与百分位的 *极端程度* 相关，也就是说 1 或 99 的百分位要比 50 百分位要准确。这只是数据结构内部机制的一种特性，但这是一个好的特性，因为多数人只关心极端的百分位。
- 对于数值集合较小的情况，百分位非常准确。如果数据集足够小，百分位可能 100% 精确。
- 随着桶里数值的增长，算法会开始对百分位进行估算。它能有效在准确度和内存节省之间做出权衡。 不准确的程度比较难以总结，因为它依赖于 聚合时数据的分布以及数据量的大小。

与 `cardinality` 类似，我们可以通过修改参数 `compression` 来控制内存与准确度之间的比值。

TDigest 算法用节点近似计算百分比：节点越多，准确度越高（同时内存消耗也越大），这都与数据量成正比。 `compression` 参数限制节点的最大数目为 `20 * compression` 。




# 导航

[目录](README.md)

上一章：[31、多桶排序](31、多桶排序.md)

下一章：[33、通过聚合发现异常指标](33、通过聚合发现异常指标.md)
