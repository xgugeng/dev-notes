<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [优化的策略](#%E4%BC%98%E5%8C%96%E7%9A%84%E7%AD%96%E7%95%A5)
- [导航](#%E5%AF%BC%E8%88%AA)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

Unix经验告诉我们最主要的就是如何知道不去优化，最强大的优化工具就是不做优化（摩尔定律）。 

如果仅仅是为了减少资源使用的一个常数部分而优化是不值得的，更明智的做法是把高次的指数级降下来。

善用profiler估量程序，并谨记工具误差，降低误差影响的一个方法是在相同负载下多次profiler。

最有效的代码优化方法是保持代码短小简单，永远不要将核心数据结构和时间关键循环抛出缓存。

性能往往受限于IO（尤其是网络程序），经验法则是尽可能降低时延：对可以共享启动开销的失误进行批处理；允许事务重叠；缓存

# 优化的策略

1.批操作

积累更新数量的测量进行批处理，

例如named面对上千万的请求，把访问都放在内存缓存，减少对磁盘的IO，减少时延。

又例如roguelike程序使用X server 和 curse每隔一个时段进行屏幕更新组织，这个如背地话话的CS模式，减少传输的时延，更新的对cpu 磁盘io的密集。



2.重叠操作

等待响应结果与阻塞 是一件痛苦的事情，重叠操作重点是异步数量返回，

例如，pop3客户端与服务器处理请求是一步一步处理，处理完一个再处理一个，IMAP协议客户端可以发出请求，每个请求都有一个标记，等待一个请求回来就立刻处理，不需要等待全部结果回来。

 

3.缓存操作结果

python解析器，运行时会把源码，编译后存到磁盘上，直到被修改，这个需要每次检测文件修改与创建的时间戳，named也是一个例子，把访问的域名映射缓存下来，问题在于，在频繁更新，会造成不稳定性，有差异，

# 导航

[目录](README.md)

上一章：[11. 接口](11. 接口.md)

下一章：[13. 复杂度](13. 复杂度.md)
