<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [硬件](#%E7%A1%AC%E4%BB%B6)
  - [内存](#%E5%86%85%E5%AD%98)
  - [CPUs](#cpus)
  - [硬盘](#%E7%A1%AC%E7%9B%98)
  - [网络](#%E7%BD%91%E7%BB%9C)
- [Java 虚拟机](#java-%E8%99%9A%E6%8B%9F%E6%9C%BA)
- [Transport Client 与 Node Client](#transport-client-%E4%B8%8E-node-client)
- [配置管理](#%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86)
- [重要配置的修改](#%E9%87%8D%E8%A6%81%E9%85%8D%E7%BD%AE%E7%9A%84%E4%BF%AE%E6%94%B9)
  - [指定名字](#%E6%8C%87%E5%AE%9A%E5%90%8D%E5%AD%97)
  - [路径](#%E8%B7%AF%E5%BE%84)
  - [最小主节点数](#%E6%9C%80%E5%B0%8F%E4%B8%BB%E8%8A%82%E7%82%B9%E6%95%B0)
  - [集群恢复的配置](#%E9%9B%86%E7%BE%A4%E6%81%A2%E5%A4%8D%E7%9A%84%E9%85%8D%E7%BD%AE)
  - [最好使用单播代替组播](#%E6%9C%80%E5%A5%BD%E4%BD%BF%E7%94%A8%E5%8D%95%E6%92%AD%E4%BB%A3%E6%9B%BF%E7%BB%84%E6%92%AD)
- [不要触碰这些配置！](#%E4%B8%8D%E8%A6%81%E8%A7%A6%E7%A2%B0%E8%BF%99%E4%BA%9B%E9%85%8D%E7%BD%AE)
  - [垃圾回收器](#%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8)
  - [线程池](#%E7%BA%BF%E7%A8%8B%E6%B1%A0)
- [堆内存:大小和交换](#%E5%A0%86%E5%86%85%E5%AD%98%E5%A4%A7%E5%B0%8F%E5%92%8C%E4%BA%A4%E6%8D%A2)
  - [将内存的（少于）一半分给 Lucene](#%E5%B0%86%E5%86%85%E5%AD%98%E7%9A%84%E5%B0%91%E4%BA%8E%E4%B8%80%E5%8D%8A%E5%88%86%E7%BB%99-lucene)
  - [不要超过32GB](#%E4%B8%8D%E8%A6%81%E8%B6%85%E8%BF%8732gb)
  - [到底需要低于 32 GB多少，来设置我的 JVM？](#%E5%88%B0%E5%BA%95%E9%9C%80%E8%A6%81%E4%BD%8E%E4%BA%8E-32-gb%E5%A4%9A%E5%B0%91%E6%9D%A5%E8%AE%BE%E7%BD%AE%E6%88%91%E7%9A%84-jvm)
  - [Swapping 是性能的坟墓](#swapping-%E6%98%AF%E6%80%A7%E8%83%BD%E7%9A%84%E5%9D%9F%E5%A2%93)
- [文件描述符和 MMap](#%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E5%92%8C-mmap)
- [导航](#%E5%AF%BC%E8%88%AA)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# 硬件

## 内存

排序和聚合都很耗内存。即使堆空间是比较小的时候， 也能为操作系统文件缓存提供额外的内存。因为 Lucene 使用的许多数据结构是基于磁盘的格式，Elasticsearch 利用操作系统缓存能产生很大效果。

64 GB 内存的机器是非常理想的， 但是32 GB 和16 GB 机器也是很常见的。少于8 GB 会适得其反（你最终需要很多很多的小机器）。

## CPUs

大多数 Elasticsearch 部署往往对 CPU 要求不高。因此， 相对其它资源，具体配置多少个（CPU）不是那么关键。你应该选择具有多个内核的现代处理器，常见的集群使用两到八个核的机器。

如果你要在更快的 CPUs 和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。

## 硬盘

硬盘是服务器上最慢的子系统，这意味着那些写入量很大的集群很容易让硬盘饱和，使得它成为集群的瓶颈。

如果你负担得起 SSD，它将远远超出任何旋转介质（注：机械硬盘，磁带等）。 基于 SSD 的节点，查询和索引性能都有提升。如果你负担得起，SSD 是一个好的选择。

> 如果你正在使用 SSDs，确保你的系统 I/O 调度程序是配置正确的。 当你向硬盘写数据，I/O 调度程序决定何时把数据实际发送到硬盘。 大多数默认 *nix 发行版下的调度程序都叫做 `cfq`（完全公平队列）。
>
> 调度程序分配 *时间片* 到每个进程。并且优化这些到硬盘的众多队列的传递。但它是为旋转介质优化的： 机械硬盘的固有特性意味着它写入数据到基于物理布局的硬盘会更高效。
>
> 这对 SSD 来说是低效的，尽管这里没有涉及到机械硬盘。但是，`deadline` 或者 `noop` 应该被使用。`deadline` 调度程序基于写入等待时间进行优化， `noop` 只是一个简单的 FIFO 队列。
>
> 这个简单的更改可以带来显著的影响。仅仅是使用正确的调度程序，我们看到了500倍的写入能力提升。

## 网络

快速可靠的网络显然对分布式系统的性能是很重要的 。 低延时能帮助确保节点间能容易的通讯，大带宽能帮助分片移动和恢复。现代数据中心网络（1 GbE, 10 GbE）对绝大多数集群都是足够的。

即使数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。

Elasticsearch 假定所有节点都是平等的--并不会因为有一半的节点在150ms 外的另一数据中心而有所不同。更大的延时会加重分布式系统中的问题而且使得调试和排错更困难。

# Java 虚拟机

Lucene 的单元测试和集成测试经常暴露出 JVM 本身的 bug。这些 bug 的范围从轻微的麻烦到严重段错误，所以，最好尽可能的使用最新版本的 JVM。

Java 8 强烈优先选择于 Java 7。不再支持 Java 6。Oracle 或者 OpenJDK 是可以接受的，它们在性能和稳定性也差不多。

如果你的应用程序是用 Java 编写并正在使用传输客户端（注：Transport Client，下同）或节点客户端（注：Node Client，下同），请确保运行你应用程序的 JVM 和服务器的 JVM 是完全一样的。 在 Elasticsearch 的几个地方，使用 Java 的本地序列化（ IP 地址、异常等等）。不幸的是，Oracle 的 JVM 在几个小版本之间有修改序列化格式，从而导致奇怪的错误。 

# Transport Client 与 Node Client

传输客户端作为一个集群和应用程序之间的通信层。它知道 API 并能自动帮你在节点之间轮询，帮你嗅探集群等等。但它是集群 *外部的* ，和 REST 客户端类似。

另一方面，节点客户端，实际上是一个集群中的节点（但不保存数据，不能成为主节点）。因为它是一个节点，它知道整个集群状态（所有节点驻留，分片分布在哪些节点，等等）。 这意味着它可以执行 APIs 但少了一个网络跃点。

这里有两个客户端案例的使用情况：

- 如果要将应用程序和 Elasticsearch 集群进行解耦，传输客户端是一个理想的选择。例如，如果您的应用程序需要快速的创建和销毁到集群的连接，传输客户端比节点客户端”轻”，因为它不是一个集群的一部分。

  类似地，如果您需要创建成千上万的连接，你不想有成千上万节点加入集群。传输客户端（ TC ）将是一个更好的选择。

- 另一方面，如果你只需要有少数的、长期持久的对象连接到集群，客户端节点可以更高效，因为它知道集群的布局。但是它会使你的应用程序和集群耦合在一起，所以从防火墙的角度，它可能会构成问题。

# 配置管理

配置管理工具（ Puppet，Chef，Ansible）通过自动化更改配置的过程保持集群的一致性。这可能需要一点时间来建立和学习，但它本身，随着时间的推移会有丰厚的回报。

# 重要配置的修改

其它数据库可能需要调优，但总得来说，Elasticsearch 不需要。 如果你遇到了性能问题，解决方法通常是更好的数据布局或者更多的节点。 在 Elasticsearch 中很少有“神奇的配置项”， 如果存在，我们也已经帮你优化了！

## 指定名字

```
cluster.name: elasticsearch_production
node.name: elasticsearch_005_data
```

## 路径

默认情况下， Elasticsearch 会把插件、日志以及你最重要的数据放在安装目录下。这会带来不幸的事故， 如果你重新安装 Elasticsearch 的时候不小心把安装目录覆盖了。

```
path.data: /path/to/data1,/path/to/data2 

# Path to log files:
path.logs: /path/to/logs

# Path to where plugins are installed:
path.plugins: /path/to/plugins
```

## 最小主节点数

`minimum_master_nodes` 设定对你的集群的稳定 *极其* 重要。 当你的集群中有两个 masters（注：主节点）的时候，这个配置有助于防止 *脑裂* ，一种两个主节点同时存在于一个集群的现象。

此设置应该始终被配置为 master 候选节点的法定个数（大多数个）。法定个数就是 `( master 候选节点个数 / 2) + 1` 。 

```
discovery.zen.minimum_master_nodes: 2
```

## 集群恢复的配置

```
gateway.recover_after_nodes: 8
```

这将阻止 Elasticsearch 在存在至少 8 个节点（数据节点或者 master 节点）之前进行数据恢复。

现在我们要告诉 Elasticsearch 集群中 *应该* 有多少个节点，以及我们愿意为这些节点等待多长时间：

```
gateway.expected_nodes: 10
gateway.recover_after_time: 5m
```

## 最好使用单播代替组播

Elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。

虽然组播仍然 [作为插件提供](https://www.elastic.co/guide/en/elasticsearch/plugins/current/discovery-multicast.html)， 但它应该永远不被使用在生产环境了，否在你得到的结果就是一个节点意外的加入到了你的生产环境，仅仅是因为他们收到了一个错误的组播信号。

```
discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]
```



# 不要触碰这些配置！

## 垃圾回收器

不要更改默认的垃圾回收器！

Elasticsearch 默认的垃圾回收器（ GC ）是 CMS。 这个垃圾回收器可以和应用并行处理，以便它可以最小化停顿。 然而，它有两个 stop-the-world 阶段，处理大内存也有点吃力。

尽管有这些缺点，它还是目前对于像 Elasticsearch 这样低延迟需求软件的最佳垃圾回收器。官方建议使用 CMS。

在有一款新的垃圾回收器，叫 G1 垃圾回收器（ G1GC ）。 这款新的 GC 被设计，旨在比 CMS 更小的暂停时间，以及对大内存的处理能力。 它的原理是把内存分成许多区域，并且预测哪些区域最有可能需要回收内存。通过优先收集这些区域（ *garbage first* ），产生更小的暂停时间，从而能应对更大的内存。

听起来很棒！遗憾的是，G1GC 还是太新了，经常发现新的 bugs。这些错误通常是段（ segfault ）类型，便造成硬盘的崩溃。 Lucene 的测试套件对垃圾回收算法要求严格，看起来这些缺陷 G1GC 并没有很好地解决。

## 线程池

Elasticsearch 默认的线程设置已经是很合理的了。对于所有的线程池（除了 `搜索` ），线程个数是根据 CPU 核心数设置的。 如果你有 8 个核，你可以同时运行的只有 8 个线程，只分配 8 个线程给任何特定的线程池是有道理的。

搜索线程池设置的大一点，配置为 `int（（ 核心数 ＊ 3 ）／ 2 ）＋ 1` 。

# 堆内存:大小和交换

Elasticsearch 默认安装后设置的堆内存是 1 GB。

最简单的一个方法就是指定 `ES_HEAP_SIZE` 环境变量。服务进程在启动时候会读取这个变量，并相应的设置堆的大小。 比如，你可以用下面的命令设置它：

```
export ES_HEAP_SIZE=10g
```

此外，你也可以通过命令行参数的形式，在程序启动的时候把内存大小传递给它，如果你觉得这样更简单的话：

```
./bin/elasticsearch -Xmx10g -Xms10g
```

## 将内存的（少于）一半分给 Lucene

内存对于 Elasticsearch 来说绝对是重要的，它可以被许多内存数据结构使用来提供更快的操作。但是说到这里， 还有另外一个内存消耗大户 *非堆内存* （off-heap）：Lucene。

Lucene 被设计为可以利用操作系统底层机制来缓存内存数据结构。 Lucene 的段是分别存储到单个文件中的。因为段是不可变的，这些文件也都不会变化，这是对缓存友好的，同时操作系统也会把这些段文件缓存起来，以便更快的访问。

Lucene 的性能取决于和操作系统的相互作用。如果你把所有的内存都分配给 Elasticsearch 的堆内存，那将不会有剩余的内存交给 Lucene。 这将严重地影响全文检索的性能。

标准的建议是把 50％ 的可用内存作为 Elasticsearch 的堆内存，保留剩下的 50％。当然它也不会被浪费，Lucene 会很乐意利用起余下的内存。

如果你不需要对分词字符串做聚合计算，可以考虑降低堆内存。堆内存越小，Elasticsearch（更快的 GC）和 Lucene（更多的内存用于缓存）的性能越好。

## 不要超过32GB

 JVM 在内存小于 32 GB 的时候会采用一个内存对象指针压缩技术。

对于 32 位的系统，意味着堆内存大小最大为 4 GB。对于 64 位的系统， 可以使用更大的内存，但是 64 位的指针意味着更大的浪费，因为你的指针本身大了。更糟糕的是， 更大的指针在主内存和各级缓存（例如 LLC，L1 等）之间移动数据的时候，会占用更多的带宽。

Java 使用一个叫作 [内存指针压缩（compressed oops）](https://wikis.oracle.com/display/HotSpotInternals/CompressedOops)的技术来解决这个问题。 它的指针不再表示对象在内存中的精确位置，而是表示 *偏移量* 。这意味着 32 位的指针可以引用 40 亿个 *对象* ， 而不是 40 亿个字节。最终， 也就是说堆内存增长到 32 GB 的物理内存，也可以用 32 位的指针表示。

一旦你越过那个神奇的 ~32 GB 的边界，指针就会切回普通对象的指针。 每个对象的指针都变长了，就会使用更多的 CPU 内存带宽，也就是说你实际上失去了更多的内存。事实上，当内存到达 40–50 GB 的时候，有效内存才相当于使用内存对象指针压缩技术时候的 32 GB 内存。

这段描述的意思就是说：即便你有足够的内存，也尽量不要 超过 32 GB。因为它浪费了内存，降低了 CPU 的性能，还要让 GC 应对大内存。

## 到底需要低于 32 GB多少，来设置我的 JVM？

遗憾的是，这需要看情况。确切的划分要根据 JVMs 和操作系统而定。 如果你想保证其安全可靠，设置堆内存为 `31 GB` 是一个安全的选择。 另外，你可以在你的 JVM 设置里添加 `-XX:+PrintFlagsFinal` 用来验证 `JVM` 的临界值，并且检查 UseCompressedOops 的值是否为 true。对于你自己使用的 JVM 和操作系统，这将找到最合适的堆内存临界值。

## Swapping 是性能的坟墓

内存交换 到磁盘对服务器性能来说是 *致命* 的。

最好的办法就是在你的操作系统中完全禁用 swap。这样可以暂时禁用：

```
sudo swapoff -a
```

如果你并不打算完全禁用 swap，也可以选择降低 `swappiness` 的值。 这个值决定操作系统交换内存的频率。 这可以预防正常情况下发生交换，但仍允许操作系统在紧急情况下发生交换。

对于大部分Linux操作系统，可以在 `sysctl` 中这样配置：

```
vm.swappiness = 1 
```

最后，如果上面的方法都不合适，你需要打开配置文件中的 `mlockall` 开关。 它的作用就是允许 JVM 锁住内存，禁止操作系统交换出去。在你的 `elasticsearch.yml` 文件中，设置如下：

```
bootstrap.mlockall: true
```

# 文件描述符和 MMap

Lucene 使用了 *大量的* 文件。 同时，Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字（注：sockets）。 所有这一切都需要足够的文件描述符。

检查 Elasticsearch，以确保它的真的起作用并且有足够的文件描述符：

```
GET /_nodes/process
```

`max_file_descriptors` 字段显示 Elasticsearch 进程可以访问的可用文件描述符数量。

Elasticsearch 对各种文件混合使用了 NioFs（ 注：非阻塞文件系统）和 MMapFs （ 注：内存映射文件系统）。请确保你配置的最大映射数量，以便有足够的虚拟内存可用于 mmapped 文件。这可以暂时设置：

```
sysctl -w vm.max_map_count=262144
```

或者你可以在 `/etc/sysctl.conf` 通过修改 `vm.max_map_count` 永久设置它。


# 导航

[目录](README.md)

上一章：[44、监控](44、监控.md)

下一章：[46、部署后](46、部署后.md)
