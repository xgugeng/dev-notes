<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [5.1 动机](#51-%E5%8A%A8%E6%9C%BA)
- [5.2 文件格式](#52-%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F)
  - [5.2.1 文本文件](#521-%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6)
  - [5.2.2 JSON](#522-json)
    - [1. 读取JSON](#1-%E8%AF%BB%E5%8F%96json)
    - [2. 保存JSON](#2-%E4%BF%9D%E5%AD%98json)
  - [5.2.3 逗号分隔值与制表符分隔值](#523-%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E5%80%BC%E4%B8%8E%E5%88%B6%E8%A1%A8%E7%AC%A6%E5%88%86%E9%9A%94%E5%80%BC)
    - [1. 读取CSV](#1-%E8%AF%BB%E5%8F%96csv)
    - [2. 保存CSV](#2-%E4%BF%9D%E5%AD%98csv)
  - [5.2.4 SequenceFiles](#524-sequencefiles)
    - [1. 读取 SequenceFile](#1-%E8%AF%BB%E5%8F%96-sequencefile)
    - [2. 保存 SequenceFile](#2-%E4%BF%9D%E5%AD%98-sequencefile)
  - [5.2.5 对象文件](#525-%E5%AF%B9%E8%B1%A1%E6%96%87%E4%BB%B6)
  - [5.2.6 Hadoop 输入输出格式](#526-hadoop-%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F)
    - [1. 读取其他Hadoop输入格式](#1-%E8%AF%BB%E5%8F%96%E5%85%B6%E4%BB%96hadoop%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F)
    - [2. 保存 Hadoop 输出格式](#2-%E4%BF%9D%E5%AD%98-hadoop-%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F)
    - [3. 非文件系统数据源](#3-%E9%9D%9E%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E6%BA%90)
    - [4. protocol buffer](#4-protocol-buffer)
  - [5.2.7 文件压缩](#527-%E6%96%87%E4%BB%B6%E5%8E%8B%E7%BC%A9)
- [5.3 文件系统](#53-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F)
  - [5.3.1 本地文件系统](#531-%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F)
  - [5.3.2 Amazon S3](#532-amazon-s3)
  - [5.3.3 HDFS](#533-hdfs)
- [5.4 Spark SQL 中的结构化数据](#54-spark-sql-%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE)
  - [5.4.1 Apache Hive](#541-apache-hive)
  - [5.4.2 JSON](#542-json)
- [5.5 数据库](#55-%E6%95%B0%E6%8D%AE%E5%BA%93)
  - [5.5.1 Java数据库连接](#551-java%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5)
  - [5.5.2 Cassandra](#552-cassandra)
  - [5.5.3 Hbase](#553-hbase)
  - [5.5.4 Elasticsearch](#554-elasticsearch)
- [导航](#%E5%AF%BC%E8%88%AA)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# 5.1 动机

Spark 支持很多种输入输出源。一部分原因是 Spark 本身是基于 Hadoop 生态圈而构建，特别是 Spark 可以通过 Hadoop MapReduce 所使用的 `InputFormat` 和 `OutputFormat` 接口访问数据，而大部分常见的文件格式与存储系统（例如 S3、HDFS、Cassandra、HBase 等）都支持这种接口。

三类常见的数据源：

- **文件格式与文件系统**。对于存储在本地文件系统或分布式文件系统（比如 NFS、HDFS、Amazon S3 等）中的数据，Spark 可以访问很多种不同的文件格式，包括文本文件、JSON、SequenceFile，以及 protocol buffer。

- **Spark SQL中的结构化数据源**。针对JSON和Apache Hive。

- **数据库与键值存储**。连接如Cassandra、HBase、Elasticsearch、JDBC源。

  ​


# 5.2 文件格式 

Spark 支持的一些常见格式：

| 格式名称          | 结构化  | 备注                                       |
| ------------- | ---- | ---------------------------------------- |
| 文本文件          | 否    | 普通的文本文件，每行一条记录                           |
| JSON          | 半结构化 | 常见的基于文本的格式；大多数库都要求每行一条记录                 |
| CSV           | 是    | 基于文本，通常在电子表格中使用                          |
| SequenceFiles | 是    | 用于键值对数据的常见Hadoop文件格式                     |
| Proto buffers | 是    | 快速、解决空间的跨语言格式                            |
| 对象文件          | 是    | 用来将Spark作业的数据存储下来以让共享的代码读取。改变类的时候它会失效，因为它依赖于Java序列化 |



## 5.2.1 文本文件

当我们将一个文本文件读取为 RDD 时，输入的每一行都会成为 RDD 的一个元素。也可以将多个完整的文本文件一次性读取为pair RDD，键是文件名，值是文件内容。

```scala
val input = sc.textFile("file:///home/holden/README.md")
```

如果想读取整个目录，可以使用`wholeTextFiles()`方法，它会返回一个pair RDD。

```scala
val input = sc.wholeTextFiles("file:///home/holden")
val result = input.mapValues(y => 
                            val nums = y.split(" ").map(x => x.toDouble)
                            nums.sum / nums.size.toDouble)
```

保存文本文件，使用`saveAsTextFile()`方法接受一个路径，将RDD的内容都输入到路径对应的文件中。Spark将传入的路径当做目录，会在目录下输出多个文件。我们不能控制数据的哪一部分输出到哪个文件中，不过有些输出格式支持控制。

```python
result.saveAsTextFile(outputFile)
```

## 5.2.2 JSON

### 1. 读取JSON

将数据作为文本文件读取，然后对 JSON 数据进行解析，这样的方法可以在所有支持的编程语言中使用。这种方法假设文件中的每一行都是一条 JSON 记录。如果你有跨行的 JSON 数据，你就只能读入整个文件，然后对每个文件进行解析。如果在你使用的语言中构建一个 JSON 解析器的开销较大，你可以使用`mapPartitions()`来重构解析器。

Scala 中使用 Jackson解析 JSON 。 

```scala 
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.databind.DeserializationFeature
...
case class Person(name: String, lovesPandas: Boolean) // 必须是顶级类
...
// 将其解析为特定的case class。使用flatMap，通过在遇到问题时返回空列表（None）
// 来处理错误，而在没有问题时返回包含一个元素的列表（Some(_)）
val result = input.flatMap(record => {
  try {
    Some(mapper.readValue(record, classOf[Person]))
  } catch {
    case e: Exception => None
  }})
```

### 2. 保存JSON

写出 JSON 文件不需要考虑格式错误的数据。使用之前将字符串RDD转为解析好的JSON数据的库，将结构化数据组成的RDD转换为字符串RDD，然后用Spark的文本文件API写出去。

```scala 
result.filter(p => P.lovesPandas).map(mapper.writeValueAsString(_))
  .saveAsTextFile(outputFile)
```

## 5.2.3 逗号分隔值与制表符分隔值

逗号分隔值（CSV）文件每行都有固定数目的字段，字段间用逗号隔开（在制表符分隔值文件，即 TSV 文件中用制表符隔开）。记录通常是一行一条，也可以跨行。CSV 文件和 TSV 文件有时支持的标准并不一致，主要是在处理换行符、转义字符、非 ASCII 字符、非整数值等方面。CSV 原生并不支持嵌套字段，所以需要手动组合和分解特定的字段。

与 JSON 中的字段不一样的是，这里的每条记录都没有相关联的字段名，只能得到对应的序号。常规做法是使用第一行中每列的值作为字段名。

### 1. 读取CSV

读取 CSV/TSV 数据和读取 JSON 数据相似，都需要先把文件当作普通文本文件来读取数据，再对数据进行处理。在Scala中使用`opencsv`库。

```scala
import Java.io.StringReader
import au.com.bytecode.opencsv.CSVReader
...
val input = sc.textFile(inputFile)
val result = input.map{ line =>
  val reader = new CSVReader(new StringReader(line));
  reader.readNext();
}
```

如果在字段中嵌有换行符，就需要完整读入每个文件，然后解析各段，

```scala
case class Person(name: String, favoriteAnimal: String)

val input = sc.wholeTextFiles(inputFile)
val result = input.flatMap{ case (_, txt) =>
  val reader = new CSVReader(new StringReader(txt));
  reader.readAll().map(x => Person(x(0), x(1)))
}
```

### 2. 保存CSV

和 JSON 数据一样，写出 CSV/TSV 数据相当简单，同样可以通过重用输出编码器来加速。由于在 CSV 中我们不会在每条记录中输出字段名，因此为了使输出保持一致，需要创建一种映射关系。一种简单做法是写一个函数，用于将各字段转为指定顺序的数组。

```scala 
pandaLovers.map(person => List(person.name, person.favoriteAnimal).toArray)
.mapPartitions{people =>
  val stringWriter = new StringWriter();
  val csvWriter = new CSVWriter(stringWriter);
  csvWriter.writeAll(people.toList)
  Iterator(stringWriter.toString)
}.saveAsTextFile(outFile)
```

## 5.2.4 SequenceFiles

SequenceFile 是由没有相对关系结构的键值对文件组成的常用 Hadoop 格式。SequenceFile 文件有同步标记，Spark可以用它来定位到文件中的某个点，然后再与记录的边界对齐。这可以让 Spark 使用多个节点高效地并行读取 SequenceFile 文件。

SequenceFile 是由 Hadoop 的 `Writable` 接口的元素组成。

### 1. 读取 SequenceFile

```scala 
val data = sc.sequenceFile(inFile, classOf[Text], classOf[IntWritable]).
  map{case (x, y) => (x.toString, y.get())}
```

### 2. 保存 SequenceFile

SequenceFile 存储的是键值对，所以要创建一个可以由可以写出到 SequenceFile 的类型构成的 Pair RDD。

我们已经进行了将许多 Scala 的原生类型转为 Hadoop Writable 的隐式转换，所以如果你要写出的是 Scala 的原生类型，可以直接调用 saveSequenceFile(path) 保存你的 PairRDD ，它会帮你写出数据。如果键和值不能自动转为 Writable 类型，或者想使用变长类型（比如 VIntWritable ），就可以对数据进行映射操作，在保存之前进行类型转换。

```scala
val data = sc.parallelize(List(("Panda", 3), ("Kay", 6), ("Snail", 2)))
data.saveAsSequenceFile(outputFile)
```

## 5.2.5 对象文件

对象文件看起来就像是对 SequenceFile 的简单封装，它允许存储只包含值的 RDD。和 SequenceFile 不一样的是，对象文件是使用 Java 序列化写出的。

对对象文件使用 Java 序列化有几个要注意的地方。首先，和普通的 SequenceFile 不同，对于同样的对象，对象文件的输出和 Hadoop 的输出不一样。其次，与其他文件格式不同的是，对象文件通常用于 Spark 作业间的通信。最后，Java 序列化有可能相当慢。

要保存对象文件，只需在 RDD 上调用 `saveAsObjectFile` 就行了。读回对象文件也相当简单：用 SparkContext 中的 `objectFile()` 函数接收一个路径，返回对应的 RDD。

## 5.2.6 Hadoop 输入输出格式

Spark 支持新旧两套 Hadoop 文件 API。

### 1. 读取其他Hadoop输入格式

`newAPIHadoopFile` 接收一个路径以及三个类。第一个类是“格式”类，代表输入格式。相似的函数 hadoopFile() 则用于使用旧的 API 实现的 Hadoop 输入格式。第二个类是键的类，最后一个类是值的类。如果需要设定额外的 Hadoop 配置属性，也可以传入一个 `conf` 对象。

`KeyValueTextInputFormat` 是最简单的 Hadoop 输入格式之一，可以用于从文本文件中读取键值对数据。每一行都会被独立处理，键和值之间用制表符隔开。这个格式存在于 Hadoop 中，所以无需向工程中添加额外的依赖就能使用它。

```scala 
val input = sc.hadoopFile[Text, Text, KeyValueTextInputFormat](inputFile).map{
  case (x, y) => (x.toString, y.toString)
```

Elephant Bird 读取 LZO 算法压缩的 JSON 文件:
```scala
val input = sc.newAPIHadoopFile(inputFile, classOf[LzoJsonInputFormat],
  classOf[LongWritable], classOf[MapWritable], conf)
// "输入"中的每个MapWritable代表一个JSON对象
```

### 2. 保存 Hadoop 输出格式

`saveAsNewHadoopFile`

### 3. 非文件系统数据源

除了 `hadoopFile()` 和 `saveAsHadoopFile()` 这一大类函数，还可以使用 `hadoopDataset/saveAsHadoopDataSet` 和 `newAPIHadoopDataset/saveAsNewAPIHadoopDataset` 来访问 Hadoop 所支持的非文件系统的存储格式。例如，许多像 HBase 和 MongoDB 这样的键值对存储都提供了用来直接读取 Hadoop 输入格式的接口。我们可以在 Spark 中很方便地使用这些格式。

### 4. protocol buffer

PB定义：

```
message Venue {
  required int32 id = 1;
  required string name = 2;
  required VenueType type = 3;
  optional string address = 4;

  enum VenueType {
    COFFEESHOP = 0;
    WORKPLACE = 1;
    CLUB = 2;
    OMNOMNOM = 3;
    OTHER = 4;
  }
}

message VenueResponse {
  repeated Venue results = 1;
}
```

Scala 中使用Elephant Bird写出 protocol buffer：

```scala
val job = new Job()
val conf = job.getConfiguration
LzoProtobufBlockOutputFormat.setClassConf(classOf[Places.Venue], conf);
val dnaLounge = Places.Venue.newBuilder()
dnaLounge.setId(1);
dnaLounge.setName("DNA Lounge")
dnaLounge.setType(Places.Venue.VenueType.CLUB)
val data = sc.parallelize(List(dnaLounge.build()))
val outputData = data.map{ pb =>
  val protoWritable = ProtobufWritable.newInstance(classOf[Places.Venue]);
  protoWritable.set(pb)
  (null, protoWritable)
}
outputData.saveAsNewAPIHadoopFile(outputFile, classOf[Text],
  classOf[ProtobufWritable[Places.Venue]],
  classOf[LzoProtobufBlockOutputFormat[ProtobufWritable[Places.Venue]]], conf)
```

## 5.2.7 文件压缩

对于大多数 Hadoop 输出格式来说，我们可以指定一种压缩编解码器来压缩数据。

于像 Spark 这样的分布式系统，我们通常会尝试从多个不同机器上一起读入数据。要实现这种情况，每个工作节点都必须能够找到一条新记录的开端。有些压缩格式会使这变得不可能，而必须要单个节点来读入所有数据，这就很容易产生性能瓶颈。可以很容易地从多个节点上并行读取的格式被称为『可分割』的格式。

| 格式     | 可分割        | 平均压缩速度 | 文本文件压缩效率 | Hadoop压缩编解码器                             | 原生   | 备注              |
| ------ | ---------- | ------ | -------- | ---------------------------------------- | ---- | --------------- |
| gzip   | 否          | 快      | 高        | `org.apache.hadoop.io.compress.GzipCodec` | 是    |                 |
| lzo    | 是（取决于所用的库） | 非常快    | 中等       | `com.hadoop.compression.lzo.LzoCodec`    | 是    | 需要在每个node上安装LZO |
| bzip2  | 是          | 慢      | 非常高      | `org.apache.hadoop.io.compress.Bzip2Codec` | 是    | 为可分割版本，使用纯Java  |
| zlib   | 否          | 慢      | 中等       | `org.aoache.hadoop.io.compress.DefaultCodec` | 是    | Hadoop默认压缩编解码器  |
| Snappy | 否          | 非常快    | 低        | `org.apache.hadoop.io.compress.SnappycodeC` | 是    |                 |




# 5.3 文件系统

## 5.3.1 本地文件系统

Spark 支持从本地文件系统中读取文件，不过它要求文件在集群中所有节点的相同路径下都可以找到 。

```scala
val rdd = sc.textFile("file:///home/holden/happypandas.gz")
```

如果文件还没有放在集群中的所有节点上，你可以在驱动器程序中从本地读取该文件而无需使用整个集群，然后再调用 parallelize 将内容分发给工作节点。不过这种方式可能会比较慢，所以推荐的方法是将文件先放到像 HDFS、NFS、S3 等共享文件系统上。

## 5.3.2 Amazon S3

要在 Spark 中访问 S3 数据，你应该首先把你的 S3 访问凭据设置为 `AWS_ACCESS_KEY_ID` 和 `AWS_SECRET_ACCESS_KEY` 环境变量。你可以从 Amazon Web Service 控制台创建这些凭据。接下来，将一个以 `s3n://` 开头的路径以 `s3n://bucket/path-within-bucket` 的形式传给 Spark 的输入方法。

如果你从 Amazon 那里得到 S3 访问权限错误，请确保你指定了访问密钥的账号对数据桶有“read”（读）和“list”（列表）的权限。Spark 需要列出桶内的内容，来找到想要读取的数据。

## 5.3.3 HDFS

在 Spark 中使用 HDFS 只需要将输入输出路径指定为 `hdfs://master:port/path` 就够了。

HDFS 协议随 Hadoop 版本改变而变化，因此如果你使用的 Spark 是依赖于另一个版本的 Hadoop 编译的，那么读取会失败。



# 5.4 Spark SQL 中的结构化数据

结构化数据指的是有结构信息 的数据——也就是所有的数据记录都具有一致字段结构的集合。

我们把一条 SQL 查询给 Spark SQL，让它对一个数据源执行查询（选出一些字段或者对字段使用一些函数），然后得到由 Row 对象组成的 RDD，每个 Row 对象表示一条记录。在Scala 中，Row 对象的访问是基于下标的。每个 Row 都有一个 `get()` 方法，会返回一个一般类型让我们可以进行类型转换。另外还有针对常见基本类型的专用 `get()` 方法（例如 `getFloat()` 、`getInt()` 、`getLong()` 、`getString()` 、`getShort()` 、`getBoolean()` 等）。

## 5.4.1 Apache Hive

Apache Hive 是 Hadoop 上的一种常见的结构化数据源。Hive 可以在 HDFS 内或者在其他存储系统上存储多种格式的表。这些格式从普通文本到列式存储格式，应有尽有。Spark SQL 可以读取 Hive 支持的任何表。

要把 Spark SQL 连接到已有的 Hive 上，你需要提供 Hive 的配置文件。你需要将 `hive-site.xml` 文件复制到 Spark 的 `./conf/` 目录下。这样做好之后，再创建出 HiveContext 对象，也就是 Spark SQL 的入口，然后你就可以使用 Hive 查询语言（HQL）来对你的表进行查询，并以由行组成的 RDD 的形式拿到返回数据，

```scala
import org.apache.spark.sql.hive.HiveContext

val hiveCtx = new org.apache.spark.sql.hive.HiveContext(sc)
val rows = hiveCtx.sql("SELECT name, age FROM users")
val firstRow = rows.first()
println(firstRow.getString(0)) // 字段0是name字段
```

## 5.4.2 JSON

读取 JSON 数据，首先需要和使用 Hive 一样创建一个 HiveContext 。（不需要安装好 Hive，也不需要 `hive-site.xml` 文件。）然后使用 `HiveContext.jsonFile` 方法来从整个文件中获取由 Row 对象组成的 RDD。除了使用整个 Row 对象，你也可以将 RDD 注册为一张表，然后从中选出特定的字段。

```json
{"user": {"name": "Holden", "location": "San Francisco"}, "text": "Nice day out today"}
{"user": {"name": "Matei", "location": "Berkeley"}, "text": "Even nicer here :)"}
```

```scala
val tweets = hiveCtx.jsonFile("tweets.json")
tweets.registerTempTable("tweets")
val results = hiveCtx.sql("SELECT user.name, text FROM tweets")
```



# 5.5 数据库

## 5.5.1 Java数据库连接

Spark 可以从任何支持 Java 数据库连接（JDBC）的关系型数据库中读取数据，包括 MySQL、Postgre 等系统。要访问这些数据，需要构建一个 `org.apache.spark.rdd.JdbcRDD` ，将 SparkContext 和其他参数一起传给它。

```scala
def createConnection() = {
  Class.forName("com.mysql.jdbc.Driver").newInstance();
  DriverManager.getConnection("jdbc:mysql://localhost/test?user=holden");
}

def extractValues(r: ResultSet) = {
  (r.getInt(1), r.getString(2))
}

val data = new JdbcRDD(sc,
  createConnection, "SELECT * FROM panda WHERE ? <= id AND id <= ?",
  lowerBound = 1, upperBound = 3, numPartitions = 2, mapRow = extractValues)
println(data.collect().toList)
```

## 5.5.2 Cassandra

DataStax 开源其用于 Spark 的 Cassandra 连接器（[https://github.com/datastax/spark-cassandra-connector](https://github.com/datastax/spark-cassandra-connector)），Spark 对 Cassandra 的支持大大提升。这个连接器目前还不是 Spark 的一部分，因此你需要添加一些额外的依赖到你的构建文件中才能使用它。Cassandra 还没有使用 Spark SQL，不过它会返回由 CassandraRow 对象组成的 RDD，这些对象有一部分方法与 Spark SQL 的 Row 对象的方法相同。

```
“com.datastax.spark" %% "spark-cassandra-connector" % "1.0.0-rc5",
"com.datastax.spark" %% "spark-cassandra-connector-java" % "1.0.0-rc5”
```

Cassandra 连接器要读取一个作业属性来决定连接到哪个集群。我们把 `spark.cassandra.connection.host` 设置为指向 Cassandra 集群。如果有用户名和密码的话，则需要分别设置 `spark.cassandra.auth.username` 和 `spark.cassandra.auth.password` 。

```scala
val conf = new SparkConf(true)
        .set("spark.cassandra.connection.host", "hostname")

val sc = new SparkContext(conf)
```

Datastax 的 Cassandra 连接器使用 Scala 中的隐式转换来为 SparkContext 和 RDD 提供一些附加函数。

```scala
// 为SparkContext和RDD提供附加函数的隐式转换
import com.datastax.spark.connector._

// 将整张表读为一个RDD。假设你的表test的创建语句为
// CREATE TABLE test.kv(key text PRIMARY KEY, value int);
val data = sc.cassandraTable("test" , "kv")
// 打印出value字段的一些基本统计。
data.map(row => row.getInt("value")).stats()
```

Cassandra 连接器支持把多种类型的RDD保存到 Cassandra 中。我们可以直接保存由 CassandraRow 对象组成的 RDD。通过指定列的映射关系，我们也可以存储不是行的形式而是元组和列表的形式的 RDD。

```scala 
val rdd = sc.parallelize(List(Seq("moremagic", 1)))
rdd.saveToCassandra("test" , "kv", SomeColumns("key", "value"))
```

## 5.5.3 Hbase

由于 `org.apache.hadoop.hbase.mapreduce.TableInputFormat` 类的实现，Spark 可以通过 Hadoop 输入格式访问 HBase。这个输入格式会返回键值对数据，其中键的类型为 `org.apache.hadoop.hbase.io.ImmutableBytesWritable` ，而值的类型为 `org.apache.hadoop.hbase.client.Result` 。

```scala
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat

val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE, "tablename") // 扫描哪张表

val rdd = sc.newAPIHadoopRDD(
  conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable],classOf[Result])
```

`TableInputFormat` 包含多个可以用来优化对 HBase 的读取的设置项，比如将扫描限制到一部分列中，以及限制扫描的时间范围。

## 5.5.4 Elasticsearch

Spark 可以使用 Elasticsearch-Hadoop（[https://github.com/elastic/elasticsearch-hadoop](https://github.com/elastic/elasticsearch-hadoop)）从 Elasticsearch 中读写数据。

Elasticsearch 连接器会忽略我们提供的路径信息，而依赖于在 SparkContext 中设置的配置项。Elasticsearch 的 `OutputFormat` 连接器也没有用到 Spark 所封装的类型，所以我们使用 `saveAsHadoopDataSet` 来代替，这意味着我们需要手动设置更多属性。

在Spark中使用Elasticsearch输出
```scala
val jobConf = new JobConf(sc.hadoopConfiguration)
jobConf.set("mapred.output.format.class", "org.elasticsearch.hadoop.
mr.EsOutputFormat")
jobConf.setOutputCommitter(classOf[FileOutputCommitter])
jobConf.set(ConfigurationOptions.ES_RESOURCE_WRITE, "twitter/tweets")
jobConf.set(ConfigurationOptions.ES_NODES, "localhost")
FileOutputFormat.setOutputPath(jobConf, new Path("-"))
output.saveAsHadoopDataset(jobConf)
```

在Spark中使用Elasticsearch输入
```scala
def mapWritableToInput(in: MapWritable): Map[String, String] = {
  in.map{case (k, v) => (k.toString, v.toString)}.toMap
}

val jobConf = new JobConf(sc.hadoopConfiguration)
jobConf.set(ConfigurationOptions.ES_RESOURCE_READ, args(1))
jobConf.set(ConfigurationOptions.ES_NODES, args(2))
val currentTweets = sc.hadoopRDD(jobConf,
  classOf[EsInputFormat[Object, MapWritable]], classOf[Object],
  classOf[MapWritable])
// 仅提取map
// 将MapWritable[Text, Text]转为Map[String, String]
val tweets = currentTweets.map{ case (key, value) => mapWritableToInput(value) }
```

# 导航

[目录](README.md)

上一章：[4. 键值对操作](4. 键值对操作.md)

下一章：[6. Spark编程进阶](6. Spark编程进阶.md)
