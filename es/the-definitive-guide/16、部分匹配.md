<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [邮编与结构化数据](#%E9%82%AE%E7%BC%96%E4%B8%8E%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE)
  - [prefix 前缀查询](#prefix-%E5%89%8D%E7%BC%80%E6%9F%A5%E8%AF%A2)
- [通配符与正则表达式查询](#%E9%80%9A%E9%85%8D%E7%AC%A6%E4%B8%8E%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%9F%A5%E8%AF%A2)
- [查询时输入即搜索](#%E6%9F%A5%E8%AF%A2%E6%97%B6%E8%BE%93%E5%85%A5%E5%8D%B3%E6%90%9C%E7%B4%A2)
- [索引时优化](#%E7%B4%A2%E5%BC%95%E6%97%B6%E4%BC%98%E5%8C%96)
- [Ngrams 在部分匹配的应用](#ngrams-%E5%9C%A8%E9%83%A8%E5%88%86%E5%8C%B9%E9%85%8D%E7%9A%84%E5%BA%94%E7%94%A8)
- [索引时输入即搜索](#%E7%B4%A2%E5%BC%95%E6%97%B6%E8%BE%93%E5%85%A5%E5%8D%B3%E6%90%9C%E7%B4%A2)
  - [准备索引](#%E5%87%86%E5%A4%87%E7%B4%A2%E5%BC%95)
  - [查询字段](#%E6%9F%A5%E8%AF%A2%E5%AD%97%E6%AE%B5)
- [Ngrams 在复合词的应用](#ngrams-%E5%9C%A8%E5%A4%8D%E5%90%88%E8%AF%8D%E7%9A%84%E5%BA%94%E7%94%A8)
- [导航](#%E5%AF%BC%E8%88%AA)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

 *部分匹配* 允许用户指定查找词的一部分并找出所有包含这部分片段的词。

当然， Elasticsearch 提供分析过程，倒排索引让我们不需要使用这种粗笨的技术。为了能应对同时匹配 “fox” 和 “foxes” 的情况，只需简单的将它们的词干作为索引形式，没有必要做部分匹配。

# 邮编与结构化数据

假设将邮编作为 `not_analyzed` 的精确值字段索引，所以可以为其创建索引，如下：

```
PUT /my_index
{
    "mappings": {
        "address": {
            "properties": {
                "postcode": {
                    "type":  "string",
                    "index": "not_analyzed"
                }
            }
        }
    }
}
```

## prefix 前缀查询

为了找到所有以 `W1` 开始的邮编，可以使用简单的 `prefix` 查询：

```
GET /my_index/address/_search
{
    "query": {
        "prefix": {
            "postcode": "W1"
        }
    }
}
```

默认状态下， `prefix` 查询不做相关度评分计算，它只是将所有匹配的文档返回，并为每条结果赋予评分值 `1` 。它的行为更像是过滤器而不是查询。 `prefix` 查询和 `prefix` 过滤器这两者实际的区别就是过滤器是可以被缓存的，而查询不行。

为了支持前缀匹配，查询会做以下事情：

1. 扫描词列表并查找到第一个以 `W1` 开始的词。
2. 搜集关联的文档 ID 。
3. 移动到下一个词。
4. 如果这个词也是以 `W1` 开头，查询跳回到第二步再重复执行，直到下一个词不以 `W1` 为止。

# 通配符与正则表达式查询

`wildcard` 通配符查询也是一种底层基于词的查询， 与前缀查询不同的是它允许指定匹配的正则式。它使用标准的 shell 通配符查询： `?` 匹配任意字符， `*` 匹配 0 或多个字符。

这个查询会匹配包含 `W1F 7HW` 和 `W2F 8HW` 的文档：

```
GET /my_index/address/_search
{
    "query": {
        "wildcard": {
            "postcode": "W?F*HW" 
        }
    }
}
```

`regexp` 正则式查询允许写出这样更复杂的模式：

```
GET /my_index/address/_search
{
    "query": {
        "regexp": {
            "postcode": "W[0-9].+" 
        }
    }
}
```

`wildcard` 和 `regexp` 查询的工作方式与 `prefix` 查询完全一样，它们也需要扫描倒排索引中的词列表才能找到所有匹配的词，然后依次获取每个词相关的文档 ID ，与 `prefix` 查询的唯一不同是：它们能支持更为复杂的匹配模式。

`prefix` 、 `wildcard` 和 `regexp` 查询是基于词操作的，如果用它们来查询 `analyzed` 字段，它们会检查字段里面的每个词，而不是将字段作为整体来处理。

# 查询时输入即搜索

对于查询时的输入即搜索，可以使用 `match_phrase` 的一种特殊形式， `match_phrase_prefix` 查询：

```
{
    "match_phrase_prefix" : {
        "brand" : "johnnie walker bl"
    }
}
```

这种查询的行为与 `match_phrase` 查询一致，不同的是它将查询字符串的最后一个词作为前缀使用，换句话说，可以将之前的例子看成如下这样：

- `johnnie`
- 跟着 `walker`
- 跟着以 `bl` 开始的词

如果通过 `validate-query` API 运行这个查询查询，explanation 的解释结果为：

```
"johnnie walker bl*"
```

与 `match_phrase` 一样，它也可以接受 `slop` 参数（参照 [slop](http://elasticsearch.cn/book/elasticsearch_definitive_guide_2.x/slop.html) ）让相对词序位置不那么严格：这种查询的行为与 `match_phrase` 查询一致，不同的是它将查询字符串的最后一个词作为前缀使用，换句话说，可以将之前的例子看成如下这样：

- `johnnie`
- 跟着 `walker`
- 跟着以 `bl` 开始的词

如果通过 `validate-query` API 运行这个查询查询，explanation 的解释结果为：

```
"johnnie walker bl*"
```

与 `match_phrase` 一样，它也可以接受 `slop` 参数让相对词序位置不那么严格。

通过设置 `max_expansions` 参数来限制前缀扩展的影响， 一个合理的值是可能是 50 ：

```
{
    "match_phrase_prefix" : {
        "brand" : {
            "query":          "johnnie walker bl",
            "max_expansions": 50
        }
    }
}
```

参数 `max_expansions` 控制着可以与前缀匹配的词的数量，它会先查找第一个与前缀 `bl` 匹配的词，然后依次查找搜集与之匹配的词（按字母顺序），直到没有更多可匹配的词或当数量超过 `max_expansions` 时结束。

当用户每多输入一个字符时，这个查询又会执行一遍，所以查询需要快，如果第一个结果集不是用户想要的，他们会继续输入直到能搜出满意的结果为止。

# 索引时优化

查询时的灵活性通常会以牺牲搜索性能为代价，有时候将这些消耗从查询过程中转移到别的地方是有意义的。

可以通过在索引时处理数据提高搜索的灵活性以及提升系统性能。为此仍然需要付出应有的代价：增加的索引空间与变慢的索引能力，但这与每次查询都需要付出代价不同，索引时的代价只用付出一次。

# Ngrams 在部分匹配的应用

可以将 *n-gram* 看成一个在词语上 *滑动窗口* ， *n* 代表这个 “窗口” 的长度。如果我们要 n-gram `quick` 这个词 —— 它的结果取决于 *n* 的选择长度：

- 长度 1（unigram）： [ `q`, `u`, `i`, `c`, `k` ]
- 长度 2（bigram）： [ `qu`, `ui`, `ic`, `ck` ]
- 长度 3（trigram）： [ `qui`, `uic`, `ick` ]
- 长度 4（four-gram）： [ `quic`, `uick` ]

# 索引时输入即搜索

设置索引时输入即搜索的第一步是需要定义好分析链。

## 准备索引

第一步需要配置一个自定义的 `edge_ngram` token 过滤器，称为 `autocomplete_filter` 。这个配置的意思是：对于这个 token 过滤器接收的任意词项，过滤器会为之生成一个最小固定值为 1 ，最大为 20 的 n-gram 。

然后会在一个自定义分析器 `autocomplete` 中使用上面这个 token 过滤器。

```
PUT /my_index
{
    "settings": {
        "number_of_shards": 1, 
        "analysis": {
            "filter": {
                "autocomplete_filter": { 
                    "type":     "edge_ngram",
                    "min_gram": 1,
                    "max_gram": 20
                }
            },
            "analyzer": {
                "autocomplete": {
                    "type":      "custom",
                    "tokenizer": "standard",
                    "filter": [
                        "lowercase",
                        "autocomplete_filter" 
                    ]
                }
            }
        }
    }
}
```

`update-mapping` API 将这个分析器应用到具体字段：

```
PUT /my_index/_mapping/my_type
{
    "my_type": {
        "properties": {
            "name": {
                "type":     "string",
                "analyzer": "autocomplete"
            }
        }
    }
}
```

## 查询字段

如果使用简单 `match` 查询测试查询 “brown fo” ：

```
GET /my_index/my_type/_search
{
    "query": {
        "match": {
            "name": "brown fo"
        }
    }
}
```

# Ngrams 在复合词的应用

TODO


# 导航

[目录](README.md)

上一章：[15、近似匹配](15、近似匹配.md)

下一章：[17、控制相关度](17、控制相关度.md)
