<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [6.2 累加器](#62-%E7%B4%AF%E5%8A%A0%E5%99%A8)
  - [6.2.1 累加器与容错性](#621-%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%AE%B9%E9%94%99%E6%80%A7)
  - [6.2.2 自定义累加器](#622-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8)
- [6.3 广播变量](#63-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F)
- [6.4 基于分区进行操作](#64-%E5%9F%BA%E4%BA%8E%E5%88%86%E5%8C%BA%E8%BF%9B%E8%A1%8C%E6%93%8D%E4%BD%9C)
- [6.5 与外部程序间的管道](#65-%E4%B8%8E%E5%A4%96%E9%83%A8%E7%A8%8B%E5%BA%8F%E9%97%B4%E7%9A%84%E7%AE%A1%E9%81%93)
- [6.6 数值 RDD 的操作](#66-%E6%95%B0%E5%80%BC-rdd-%E7%9A%84%E6%93%8D%E4%BD%9C)
- [导航](#%E5%AF%BC%E8%88%AA)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

两种类型的共享变量：累加器 （accumulator）与广播变量 （broadcast variable）。累加器用来对信息进行聚合，而广播变量用来高效分发较大的对象。

# 6.2 累加器

通常在向 Spark 传递函数时，比如使用 `map()` 函数或者用 `filter()` 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。Spark 的两个共享变量，累加器 与广播变量 ，分别为结果聚合与广播这两种常见的通信模式突破了这一限制。

累加器，提供了将工作节点中的值聚合到驱动器程序中的简单语法。累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。

```scala
val sc = new SparkContext(...)
val file = sc.textFile("file.txt")

val blankLines = sc.accumulator(0) // 创建Accumulator[Int]并初始化为0

val callSigns = file.flatMap(line => {
  if (line == "") {
    blankLines += 1 // 累加器加1
  }
  line.split(" ")
})

callSigns.saveAsTextFile("output.txt")
println("Blank lines: " + blankLines.value)`
```

总结起来，累加器的用法如下所示。

- 通过在驱动器中调用 `SparkContext.accumulator(initialValue)` 方法，创建出存有初始值的累加器。返回值为 `org.apache.spark.Accumulator[T]` 对象，其中 `T` 是初始值 `initialValue` 的类型。
- Spark 闭包里的执行器代码可以使用累加器的 `+=` 方法增加累加器的值。
- 驱动器程序可以调用累加器的 `value` 属性来访问累加器的值。
- 工作节点上的任务不能访问累加器的值。从这些任务的角度来看，累加器是一个*只写*变量。

## 6.2.1 累加器与容错性

Spark 会自动重新执行失败的或较慢的任务来应对有错误的或者比较慢的机器。这种情况下，对于要在*行动操作*中使用的累加器，Spark只会把每个任务对各累加器的修改应用一次 。因此，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在 `foreach()` 这样的行动操作中。

对于在 RDD 转化操作中使用的累加器，就不能保证有这种情况了。转换操作中累加器可能会发生不止一次更新。

## 6.2.2 自定义累加器

Spark 还直接支持 `Double` 、`Long` 和 `Float` 型的累加器。除此以外，Spark 也引入了自定义累加器和聚合操作的 API。自定义累加器需要扩展 `AccumulatorParam`。


# 6.3 广播变量

**广播变量**可以让程序高效地向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。

广播变量其实就是类型为 `spark.broadcast.Broadcast[T]` 的一个对象，其中存放着类型为 `T` 的值。可以在任务中通过对 `Broadcast` 对象调用 `value` 来获取该对象的值。这个值只会被发送到各节点一次，使用的是一种高效的类似 BitTorrent 的通信机制。

```scala 
// 查询RDD contactCounts中的呼号的对应位置。将呼号前缀
// 读取为国家代码来进行查询
val signPrefixes = sc.broadcast(loadCallSignTable())
val countryContactCounts = contactCounts.map{case (sign, count) =>
  val country = lookupInArray(sign, signPrefixes.value)
  (country, count)
```

使用广播变量的过程很简单: 

- 通过对一个类型 `T` 的对象调用 `SparkContext.broadcast` 创建出一个 `Broadcast[T]` 对象。任何可序列化的类型都可以这么实现。
- 通过` value` 属性访问该对象的值。
- 变量只会被发到各个节点一次，应作为*只读*值处理（修改这个值不会影响到别的节点）。

当广播一个比较大的值时，选择既快又好的序列化格式是很重要的。Spark 的 Scala 中默认使用的序列化库为 Java 序列化库，因此它对于除基本类型的数组以外的任何对象都比较低效。你可以使用 `spark.serializer` 属性选择另一个序列化库来优化序列化过程，也可以为你的数据类型实现自己的序列化方式。


# 6.4 基于分区进行操作

基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的工作。Spark 提供基于分区 的 `map` 和 `foreach` ，让你的部分代码只对 RDD 的每个分区运行一次，这样可以帮助降低这些操作的代价。

在 Scala 中使用共享连接池与 JSON 解析器: 
```scala 
val contactsContactLists = validSigns.distinct().mapPartitions{
  signs =>
  val mapper = createMapper()
  val client = new HttpClient()
  client.start()
  // 创建http请求
  signs.map {sign =>
    createExchangeForSign(sign)
 // 获取响应
  }.map{ case (sign, exchange) =>
      (sign, readExchangeCallLog(mapper, exchange))
  }.filter(x => x._2 != null) // 删除空的呼叫日志
}
```

| 函数                         | 调用所提供的              | 返回的       | 对`RDD[T]`的函数签名                         |
| -------------------------- | ------------------- | --------- | -------------------------------------- |
| `mapPartitions()`          | 该分区中元素的迭代器          | 返回的元素的迭代器 | `f: (Iterator[T]) -> Iterator[U]`      |
| `mapPartitionsWithIndex()` | 分区序号，以及每个分区中的元素的迭代器 | 返回的元素的迭代器 | `f: (Int, Iterator[T]) -> Iterator[U]` |
| `foreachPartitions()`      | 元素迭代器               | 无         | `f: (Iterator[T]) -> Unit`             |



# 6.5 与外部程序间的管道

如果 Scala、Java 以及 Python 都不能实现你需要的功能，那么 Spark 也为这种情况提供了一种通用机制，可以将数据通过管道传给用其他语言编写的程序，比如 R 语言脚本。

Spark 在 RDD 上提供 `pipe()` 方法可以让我们使用任意一种语言实现 Spark 作业中的部分逻辑，只要它能读写 *Unix标准流* 就行。通过`pipe()` ，你可以将 RDD 中的各元素从标准输入流中以字符串形式读出，并对这些元素执行任何你需要的操作，然后把结果以字符串的形式写入标准输出 —— 这个过程就是 RDD 的转化操作过程。

在 Scala 中使用 `pipe()` 调用 `finddistance.R` 的驱动器程序: 

```scala 
// 使用一个R语言外部程序计算每次呼叫的距离
// 将脚本添加到各个节点需要在本次作业中下载的文件的列表中
val distScript = "./src/R/finddistance.R"
val distScriptName = "finddistance.R"
sc.addFile(distScript)
val distances = contactsContactLists.values.flatMap(x => x.map(y =>
  s"$y.contactlay,$y.contactlong,$y.mylat,$y.mylong")).pipe(Seq(
    SparkFiles.get(distScriptName)))
println(distances.collect().toList)
```

# 6.6 数值 RDD 的操作

Spark 的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用 stats() 时通过一次遍历数据计算出来，并以 StatsCounter 对象返回。

| 方法                 | 含义         |
| ------------------ | ---------- |
| `count()`          | RDD 中的元素个数 |
| `mean()`           | 元素的平均值     |
| `sum()`            | 总和         |
| `max()`            | 最大值        |
| `min()`            | 最小值        |
| `variance()`       | 元素的方差      |
| `sampleVariance()` | 从采样中计算的方差  |
| `stdev()`          | 标准差        |
| `sampleStdev()`    | 从采样中计算的标准差 |

```scala
// 现在要移除一些异常值，因为有些地点可能是误报的
// 首先要获取字符串RDD并将它转换为双精度浮点型
val distanceDouble = distance.map(string => string.toDouble)
val stats = distanceDoubles.stats()
val stddev = stats.stdev
val mean = stats.mean
val reasonableDistances = distanceDoubles.filter(x => math.abs(x-mean) < 3 * stddev)
println(reasonableDistance.collect().toList)
```

# 导航

[目录](README.md)

上一章：[5. 数据读取与保存](5. 数据读取与保存.md)

下一章：[7. 在集群上运行Spark](7. 在集群上运行Spark.md)
