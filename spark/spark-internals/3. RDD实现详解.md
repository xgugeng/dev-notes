<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [3.1 概述](#31-%E6%A6%82%E8%BF%B0)
- [3.2 什么是 RDD](#32-%E4%BB%80%E4%B9%88%E6%98%AF-rdd)
  - [3.2.1 RDD 的创建](#321-rdd-%E7%9A%84%E5%88%9B%E5%BB%BA)
  - [3.2.2 RDD 的转换](#322-rdd-%E7%9A%84%E8%BD%AC%E6%8D%A2)
  - [3.2.4 RDD 的缓存](#324-rdd-%E7%9A%84%E7%BC%93%E5%AD%98)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

RDD 是 Spark 最基本的抽象。它具备容错性、基于内存。

# 3.1 概述

分布式数据集的容错性有两种方式实现：

- 数据检查点。通过网络连接在机器之间复制数据，而网络带宽比内存带宽低很多。
- 记录数据的更新。RDD 选择此种方式，如果更新太多，记录更新的成本也不低。所以RDD只支持粗粒度转换，即在大量纪录上执行的单个操作。

# 3.2 什么是 RDD

RDD是只读的、分区记录的集合。 RDD 只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建（转换）。RDD 含有如何从其他RDD中衍生出自己的相关信息，可以在部分分区数据丢失的时候，从物理存储的数据中计算出相应的RDD分区。

每个RDD有以下主要属性：

1. 一组分片（partition），即数据集的基本组成单位。每个分片都会被一个计算任务处理，并决定并行计算的粒度。

   默认的分片个数是程序所分配的CPU核心数，用户可以修改。每个分配的存储是由`BlockManager`实现，每个分区都会被逻辑映射为一个Block，这个Block会被一个Task负责计算。

2. 一个计算每个分区的函数。

3. RDD之间的依赖关系。

4. 一个Partitioner，即RDD的分片函数。Spark实现了两种分片函数，一个是基于哈希的`HashPartitioner`，另一个是基于范围的`RangePartitioner`。只有对key-value的RDD，才有Partitioner。Partitioner不仅决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出的分片数量。

5. 一个列表，存储存取每个Partition的优先位置。这是遵循『移动数据步不如移动计算』的理念。

## 3.2.1 RDD 的创建

有两种方式可以创建RDD：

1. 从一个已经存在的Scala集合。
2. 由外部存储系统的数据集，包括本地文件系统、HDFS、HBase、Cassandra、S3等。

RDD 支持两种操作：

1. 转换（transformation），即从现有的数据集创建一个新的数据集。
2. 动作（action），即在数据集上进行计算后，返回一个值给Driver。

## 3.2.2 RDD 的转换

RDD中的转换都是惰性的，它们不会立即计算结果，而是发生一个action时，这些转换才被执行。使用`persist`或者`cache`方法，可以在内存中持久化一个RDD，下次查询这个RDD时能够更快访问它。

RDD 支持的转换

| 转换                                       | 含义                                       |
| ---------------------------------------- | ---------------------------------------- |
| `map(func)`                              | 返回一个新的RDD，该RDD由每一个输入元素经过`func`函数转换后组成    |
| `filter(func)`                           | 返回一个新的RDD，该RDD由经过`func`函数计算后返回值为`true`的输入元素组成 |
| `flatMap(func)`                          | 类似于`map`，但每个输入元素可以被映射为0或多个输出元素           |
| `mapPartitions(func)`                    | 类似于`map`，但独立地在RDD的每一个分片上运行。因此在类型为T的RDD上运行时，`func`的函数类型必须是`Iterator[T] => Iterator[U]` |
| `mapPartitionsWithSplit(func)`           | 类似于`mapPartitions`，但`func`带有一个证书参数表示分片的索引值。因此在类型为T的RDD上运行时，`func`的函数类型必须是`(Int, Iterator[T]) => Iterator[U]` |
| `sample(withReplacement, fraction, seed)` | 根据`fraction`指定的比例对数据进行采样，可以选择是否用随机数进行替换，`seed`用于指定随机数生产器种子 |
| `union(otherDataset)`                    | 返回一个新的RDD，新RDD是源RDD和参数RDD的联合             |
| `join(otherDataset, [numTasks])`         | 在`(K, V)`和`(K, W)`上调用，返回(`K, (V, W))`    |
| `cogroup(otherDataset, [numTasks])`      | 在`(K, V)`和`(K, W)`上调用，返回`(K, (Seq(V), Seq(W))` |
| `cartesian(ohterDataset)`                | 笛卡尔积，在`T`和`U`类型的RDD上调用，返回一个`(T, U)`      |
| `dictinct([numTasks])`                   | 返回一个包含源RDD中所有不重复元素的新RDD                  |
| `groupByKey([numTasks])`                 | 在pair RDD上调用，返回一个`(K, Seq[V]) `          |
| `reduceByKey(func, [numTasks])`          | 在pair RDD上调用，返回一个`(K, V)`，使用指定的`reduce`函数，将相同`key`的值聚合到一起。 |
| `sortByKey([ascending], [numTasks])`     | 在一个`(K, V)`上调用，K必须实现`Ordered`接口，返回一个按照`key`排序的(K, V) |

RDD 的动作：

| 动作                                       | 含义                                       |
| ---------------------------------------- | ---------------------------------------- |
| `reduce(func)`                           | 通过`func`函数聚合RDD中的所有元素。                   |
| `collect()`                              | 返回RDD中所有元素组成的数组。                         |
| `count()`                                | 返回RDD中的元素个数。                             |
| `first()`                                | 返回RDD的第一个元素。                             |
| `take(n)`                                | 返回RDD中前n个元素组成的数组。                        |
| `takeSample(withReplacement, num, seed)` | 返回一个数组，由RDD中随机采样的`num`个元素组成。             |
| `saveAsTextFile(path)`                   | 将RDD的元素以文本格式保存。对于每个元素，Spark会调用`toString`方法。 |
| `saveAsSequenceFile(path)`               | 将RDD的元素以Sequence File格式保存。指向与key-value对组成，并实现了`Writable`的接口，或者可以隐式转换为`Writable`的RDD。 |
| `countByKey()`                           | 对pair RDD有效，返回一个`(K, Int)`的map           |
| `foreach(func)`                          | 对RDD的每一个元素，执行`func`函数。                   |

## 3.2.4 RDD 的缓存



