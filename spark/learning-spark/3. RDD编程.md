<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [3.1 RDD基础](#31-rdd%E5%9F%BA%E7%A1%80)
- [3.2 创建RDD](#32-%E5%88%9B%E5%BB%BArdd)
- [3.3 RDD操作](#33-rdd%E6%93%8D%E4%BD%9C)
  - [3.3.1 转化操作](#331-%E8%BD%AC%E5%8C%96%E6%93%8D%E4%BD%9C)
  - [3.3.2 行动操作](#332-%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C)
  - [3.3.3 惰性求值](#333-%E6%83%B0%E6%80%A7%E6%B1%82%E5%80%BC)
- [3.4 向Spark传递函数](#34-%E5%90%91spark%E4%BC%A0%E9%80%92%E5%87%BD%E6%95%B0)
- [3.5 常见的转换操作和行动操作](#35-%E5%B8%B8%E8%A7%81%E7%9A%84%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C%E5%92%8C%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C)
  - [3.5.1 基本RDD](#351-%E5%9F%BA%E6%9C%ACrdd)
    - [1. 针对各个元素的转换操作](#1-%E9%92%88%E5%AF%B9%E5%90%84%E4%B8%AA%E5%85%83%E7%B4%A0%E7%9A%84%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C)
    - [2. 伪集合操作](#2-%E4%BC%AA%E9%9B%86%E5%90%88%E6%93%8D%E4%BD%9C)
    - [3. 行动操作](#3-%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C)
  - [3.5.2 在不同RDD类型间转换](#352-%E5%9C%A8%E4%B8%8D%E5%90%8Crdd%E7%B1%BB%E5%9E%8B%E9%97%B4%E8%BD%AC%E6%8D%A2)
- [3.6 持久化（缓存）](#36-%E6%8C%81%E4%B9%85%E5%8C%96%E7%BC%93%E5%AD%98)
- [导航](#%E5%AF%BC%E8%88%AA)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

RDD是分布式的元素集合。在Spark中，对数据的所有操作无外乎是创建RDD、转换已有的RDD以及调用RDD操作进行求值。在这一切的背后，Spark会自动将RDD中的数据分发到集群上，并将操作并行化执行。

# 3.1 RDD基础

Spark中的RDD是一个不可变的分布式对象集合。每个RDD都被分为多个分区，这些分区运行在集群中的不同节点上。RDD可以包含Python、Java、Scala中的任意类型的对象，甚至可以包含用户自定义的对象。

用户有两种方法创建RDD：

- 读取一个外部数据集（如 `lines = sc.textFile("README.md"）`）
- 在驱动程序里分发驱动程序的对象集合（如`list`和`set`）

创建之后，RDD支持两种类型的操作：

- 转换操作。由一个 RDD 生成一个新的 RDD。例如：

  ```
  >>> pythonLines = lines.filter(lambda line: "Python" in line)
  ```

- 行动操作。对 RDD 计算出一个结果，并把结果返回到驱动器程序中，或把结果存储到外部存储系统（如 HDFS）中。

  ```
  >>> pythonLines.first()
  u'## Interactive Python Shell
  ```


虽然你可以在任何时候定义新的 RDD，但 Spark 只会惰性 计算这些 RDD。它们只有第一次在一个行动操作中用到时，才会真正计算**。

最后，默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 `RDD.persist()` 让 Spark 把这个 RDD 缓存下来。

```
>>> pythonLines.persist

>>> pythonLines.count()
2

>>> pythonLines.first()
u'## Interactive Python Shell'
```

总的来说，每个 Spark 程序或 shell 会话都按如下方式工作：

1. 从外部数据创建出输入 RDD。
2. 使用诸如 filter() 这样的转化操作对 RDD 进行转化，以定义新的 RDD。
3. 告诉 Spark 对需要被重用的中间结果 RDD 执行 persist() 操作。
4. 使用行动操作（例如 count() 和 first() 等）来触发一次并行计算，Spark 会对计算进行优化后再执行。

# 3.2 创建RDD

创建 RDD 最简单的方式就是把程序中一个已有的集合传给 `SparkContext` 的` parallelize() `方法。

```scala
val lines = sc.parallelize(List("pandas", "i like pandas"))

```

更常用的方式是从外部存储中读取数据来创建 RDD。

```scala
val lines = sc.textFile("/path/to/README.md")
```

# 3.3 RDD操作

RDD 的转化操作是返回一个新的 RDD 的操作，比如 `map()` 和 `filter()` ，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和 first() 。

转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。

## 3.3.1 转化操作

RDD 的转化操作是返回新 RDD 的操作。转化出来的 RDD 是惰性求值的，只有在行动操作中用到这些 RDD 时才会被计算。

```scala
val inputRDD = sc.textFile("log.txt")
val errorsRDD = inputRDD.filter(line => lins.contains("error"))
val warningsRDD = inputRDD.filter(line => line.contains("warning"))
val badLinesRDD = errorsRDD.union(warningsRDD)
```

`filter()` 操作不会改变已有的 inputRDD 中的数据。实际上，该操作会返回一个全新的 RDD。而`union()`可以操作两个RDD。

通过转化操作，你从已有的 RDD 中派生出新的 RDD，Spark 会使用谱系图 （lineage graph）来记录这些不同 RDD 之间的依赖关系。Spark 需要用这些信息来按需计算每个 RDD，也可以依靠谱系图在持久化的 RDD 丢失部分数据时恢复所丢失的数据。



## 3.3.2 行动操作

行动操作是第二种类型的 RDD 操作，它们会把最终求得的结果返回到驱动器程序，或者写入外部存储系统中。由于行动操作需要生成实际的输出，它们会强制执行那些求值必须用到的 RDD 的转化操作。

```scala
println("Input had " + badLinesRDD.count() + " concerning lines")
println("Here are 10 examples:")
badLinesRDD.take(10).foreach(println)
```

RDD 还有个`collect()`函数，可以获取整个RDD中的数据。但如果数据集很大，最好不要用`collect()`。

我们通常要把数据写到诸如 HDFS 或 Amazon S3 这样的分布式的存储系统中。你可以使用 `saveAsTextFile()` 、`saveAsSequenceFile()` ，或者任意的其他行动操作来把 RDD 的数据内容以各种自带的格式保存起来。”

## 3.3.3 惰性求值

惰性求值意味着当我们对 RDD 调用转化操作（例如调用 `map()` ）时，操作不会立即执行。相反，Spark 会在内部记录下所要求执行的操作的相关信息。我们不应该把 RDD 看作存放着特定数据的数据集，而最好把每个 RDD 当作我们通过转化操作构建出来的、记录如何计算数据的指令列表。把数据读取到 RDD 的操作也同样是惰性的。因此，当我们调用 `sc.textFile()` 时，数据并没有读取进来，而是在必要时才会读取。和转化操作一样的是，读取数据的操作也有可能会多次执行。

# 3.4 向Spark传递函数

Spark 的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。

在 Scala 中，我们可以把定义的内联函数、方法的引用或静态方法传递给 Spark。传递的函数及其引用的数据需要是可序列化的（实现了 Java 的 Serializable 接口）。除此以外，传递一个对象的方法或者字段时，会包含对整个对象的引用。

```scala
class SearchFunctions(val query: String) {
  def isMatch(s: String): Boolean = {
    s.contains(query)
  }
  def getMatchesFunctionReference(rdd: RDD[String]): RDD[String] = {
    // 问题："isMatch"表示"this.isMatch"，因此我们要传递整个"this"
    rdd.map(isMatch)
  }
  def getMatchesFieldReference(rdd: RDD[String]): RDD[String] = {
    // 问题："query"表示"this.query"，因此我们要传递整个"this"
    rdd.map(x => x.split(query))
  }
  def getMatchesNoReference(rdd: RDD[String]): RDD[String] = {
    // 安全：只把我们需要的字段拿出来放入局部变量中
    val query_ = this.query
    rdd.map(x => x.split(query_))
  }
}
```

# 3.5 常见的转换操作和行动操作

## 3.5.1 基本RDD

### 1. 针对各个元素的转换操作

转化操作 `map()` 接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果 RDD 中对应元素的值。

转化操作 `filter()` 则接收一个函数，并将 RDD 中满足该函数的元素放入新的 RDD 中返回。

![img/chap1/img2.png]

```scala
val input = sc.parallelize(List(1, 2, 3, 4))
val result = input.map(x => x * x)
println(result.collect().mkString(","))
```

`flatMap()`函数被分别应用到了输入 RDD 的每个元素上。不过返回的不是一个元素，而是一个返回值序列的迭代器。可以把 `flatMap()`看作将返回的迭代器“拍扁”，这样就得到了一个由各列表中的元素组成的 RDD，而不是一个由列表组成的 RDD。


```scala
val lines = sc.parallelize(List("hello world", "hi"))
val words = lines.flatMap(line => line.split(" "))
words.first() // 返回"hello
```

![](img/chap1/img3.png)

### 2. 伪集合操作

![](img/chap1/img4.png)

- `RDD.distinct()` 转化操作来生成一个只包含不同元素的新 RDD。不过需要注意，`distinct()` 操作的开销很大，因为它需要将所有数据通过网络进行混洗（shuffle），以确保每个元素都只有一份。
- `union(other)` ，它会返回一个包含两个 RDD 中所有元素的 RDD。如果输入的 RDD 中有重复数据，`union()`操作也会包含这些重复数据
- `intersection()`只返回两个 RDD 中都有的元素。`intersection()` 在运行时也会去掉所有重复的元素（单个 RDD 内的重复元素也会一起移除）。`intersection()`比`union()` 的性能却要差很多，因为它需要通过网络混洗数据来发现共有的元素。
- `subtract(other)` 函数接收另一个RDD作为参数，返回一个由只存在于第一个 RDD 中而不存在于第二个 RDD 中的所有元素组成的 RDD。和 `intersection()` 一样，它也需要数据混洗。
- `cartesian(other)` 转化操作会返回所有可能的 (a, b) 对，其中 a 是源 RDD 中的元素，而 b 则来自另一个 RDD。

![](img/chap1/img5.png)

### 3. 行动操作

`reduce()` 接收一个函数作为参数，这个函数要操作两个 RDD 的元素类型的数据并返回一个同样类型的新元素。

```scala
val sum = rdd.reduce((x, y) => x + y)
```

`fold()` 和 `reduce()` 类似，接收一个与 `reduce()` 接收的函数签名相同的函数，再加上一个“初始值”来作为每个分区第一次调用时的结果。你所提供的初始值应当是你提供的操作的单位元素；也就是说，使用你的函数对这个初始值进行多次计算都不会改变结果。

**`fold()` 和 `reduce()` 都要求函数的返回值类型需要和我们所操作的 RDD 中的元素类型相同。**

`aggregate()` 函数则把我们从返回值类型必须与所操作的 RDD 类型相同的限制中解放出来。与 `fold()` 类似，使用 `aggregate()` 时，需要提供我们期待返回的类型的初始值。然后通过一个函数把 RDD 中的元素合并起来放入累加器。考虑到每个节点是在本地进行累加的，最终，还需要提供第二个函数来将累加器两两合并。

```scala
val result = input.aggregate((0, 0))(
               (acc, value) => (acc._1 + value, acc._2 + 1),
               (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2))
val avg = result._1 / result._2.toDouble
```

把数据返回驱动器程序中最简单、最常见的操作是 `collect()` ，它会将整个 RDD 的内容返回。

`take(n)` 返回 RDD 中的 n 个元素，并且尝试只访问尽量少的分区，因此该操作会得到一个不均衡的集合。需要注意的是，这些操作返回元素的顺序与你预期的可能不一样。

如果为数据定义了顺序，就可以使用 `top()` 从 RDD 中获取前几个元素。`top()` 会使用数据的默认顺序，但我们也可以提供自己的比较函数，来提取前几个元素。

`takeSample(withReplacement, num, seed)` 函数可以让我们从数据中获取一个采样，并指定是否替换。

有时我们会对 RDD 中的所有元素应用一个行动操作，但是不把任何结果返回到驱动器程序中，这可以使用 `foreach()` 行动操作来对 RDD 中的每个元素进行操作，而不需要把 RDD 发回本地。

`count() `用来返回元素的个数，而 `countByValue()` 则返回一个从各值到值对应的计数的映射表。

对一个数据位{1, 2, 3, 3}的RDD进行基本的 RDD 行动操作：

| 函数表                                      | 目的                         | 示例                                       | 结果                       |
| ---------------------------------------- | -------------------------- | ---------------------------------------- | ------------------------ |
| `collect()`                              | 返回RDD中的所有元素                | `rdd.collect()`                          | {1, 2, 3, 3}             |
| `count()`                                | RDD中的元素个数                  | `rdd.count()`                            | 4                        |
| `countByValue()`                         | 各元素在RDD中出现的次数              | `rdd.countByValue()`                     | {(1, 1), (2, 1), {3, 2}} |
| `take(num)`                              | 从RDD中返回num个元素              | `rdd.take(2)`                            | {1, 2}                   |
| `top(num)`                               | 从RDD中返回最前面的num个元素          | `rdd.top(2)`                             | {3, 3}                   |
| `takeOrdered(num)(ordering)`             | 从RDD中按照提供的顺序返回num个元素       | `rdd.takeOrdered(2)(myOrdering)`         | {3, 3}                   |
| `takeSample(withReplaceMent, num, [seed])` | 从RDD中任意返回一些元素              | `rdd.takeSample(false, 1)`               | 非确定的                     |
| `reduce(func)`                           | 并行整合RDD中所有元素               | `rdd.reduce((x, y) => x + y)`            | 9                        |
| `fold(zero)(func)`                       | 和`reduce()`一样，但要提供初始值      | `rdd.fold(0)((x, y) => x + y)`           | 9                        |
| `aggregate(zeroValue)(seqOp, comOp)`     | 和`reduce()`一样，但通常返回不同类型的函数 | `rdd.aggregate((0, 0)) ((x, y) => (x.\_1 + y, x.\_2 + 1), (x, y) => (x.\_1 + y.\_q1, x.\_2 + y.\_2))` | (9, 4)                   |
| `foreach(func)`                          | 对RDD中的每个元素使用给定的函数          | `rdd.foreach(func)`                      | 无                        |

## 3.5.2 在不同RDD类型间转换

在 Scala 中，将 RDD 转为有特定函数的 RDD（比如在 RDD[Double] 上进行数值操作）是由隐式转换来自动处理的。我们需要加上 `import org.apache.spark.SparkContext._` 来使用这些隐式转换。



# 3.6 持久化（缓存）

Spark RDD 是惰性求值的，而有时我们希望能多次使用同一个 RDD。如果简单地对 RDD 调用行动操作，Spark 每次都会重算 RDD 以及它的所有依赖。

Scala 中的两次执行:

```scala
val result = input.map(x => x*x)
println(result.count())
println(result.collect().mkString(","))

```

为了避免多次计算同一个 RDD，可以让 Spark 对数据进行持久化。当我们让Spark持久化存储一个RDD时，计算出RDD的节点会分别保存它们所求出的分区数据。

在 Scala 中，默认情况下 `persist()` 会把数据以序列化的形式缓存在 JVM 的堆空间中。当我们把数据写到磁盘或者堆外存储上时，也总是使用序列化后的数据。

```scala
val result = input.map(x => x * x)
result.persist(StorageLevel.DISK_ONLY)
println(result.count())
println(result.collect().mkString(","))
```

如果要缓存的数据太多，内存中放不下，Spark 会自动利用最近最少使用（LRU）的缓存策略把最老的分区从内存中移除。

RDD 还有一个方法叫作 unpersist() ，调用该方法可以手动把持久化的 RDD 从缓存中移除。


# 导航

[目录](README.md)

上一章：[2. Spark下载与入门](2. Spark下载与入门.md)

下一章：[4. 键值对操作](4. 键值对操作.md)
